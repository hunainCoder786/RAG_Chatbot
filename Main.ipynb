{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bm46ZthKE4kK"
      },
      "outputs": [],
      "source": [
        "!pip install pinecone sentence-transformers pypdf openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_mcoXKZQQqC"
      },
      "outputs": [],
      "source": [
        "from pypdf import PdfReader\n",
        "\n",
        "print()\n",
        "\n",
        "def extract_text(pdf_path):\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() + \"\\n\"\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ny_QNsFQcz9"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text, chunk_size=500, overlap=50):\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunk = text[start:end]\n",
        "        chunks.append(chunk)\n",
        "        start += chunk_size - overlap\n",
        "    return chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljj2dVUZQ1si"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BODXEKx3Q3my"
      },
      "outputs": [],
      "source": [
        "# !pip install pinecone\n",
        "\n",
        "# pcsk_2w4gyj_7amhP7kP21465hLjuA4UZVxrSCB4KaaNwgWECE8yywZx8DhYNAreiZTsbv6hv1A\n",
        "from pinecone import Pinecone,ServerlessSpec\n",
        "\n",
        "pc = Pinecone(api_key=\"pcsk_2w4gyj_7amhP7kP21465hLjuA4UZVxrSCB4KaaNwgWECE8yywZx8DhYNAreiZTsbv6hv1A\")\n",
        "\n",
        "\n",
        "index_name = \"pdf-rag-index\"\n",
        "\n",
        "# Create index only if not exists\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=384,  # because MiniLM has 384-dim embeddings\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        "    )\n",
        "\n",
        "index = pc.Index(index_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COBDAcrhTGt9"
      },
      "outputs": [],
      "source": [
        "def retrieve_chunks(query, top_k=5):\n",
        "    query_emb = embedder.encode(query).astype(np.float32).tolist()\n",
        "\n",
        "    response = index.query(\n",
        "        vector=query_emb,\n",
        "        top_k=top_k,\n",
        "        include_metadata=True\n",
        "    )\n",
        "\n",
        "    chunks = [m[\"metadata\"][\"text\"] for m in response[\"matches\"]]\n",
        "    return chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78Ah_TJATLZq"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "llm = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-HdWdb-Tndj"
      },
      "outputs": [],
      "source": [
        "def build_safe_prompt(query, chunks, max_input_tokens=512):\n",
        "    base = (\n",
        "        \"Use the context to answer the question.\\n\"\n",
        "        \"If the answer is not in the  context, say 'I donâ€™t know.'\\n\\n\"\n",
        "        \"Context:\\n\"\n",
        "    )\n",
        "\n",
        "    used = []\n",
        "    for c in chunks:\n",
        "        temp_context = \"\\n\\n\".join(used + [c])\n",
        "        temp_prompt = f\"{base}{temp_context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "\n",
        "        tok_len = len(tok(temp_prompt)[\"input_ids\"])\n",
        "\n",
        "        if tok_len <= max_input_tokens - 50:\n",
        "            used.append(c)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    final_context = \"\\n\\n\".join(used)\n",
        "    final_prompt = f\"{base}{final_context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "\n",
        "    return final_prompt\n",
        "\n",
        "\n",
        "\n",
        "def answer_question(query):\n",
        "    chunks = retrieve_chunks(query, top_k=5)\n",
        "    prompt = build_safe_prompt(query, chunks, max_input_tokens=512)\n",
        "\n",
        "    tokens = tok(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "\n",
        "    output = llm.generate(\n",
        "        **tokens,\n",
        "        max_new_tokens=150,\n",
        "        do_sample=False\n",
        "    )\n",
        "\n",
        "    return tok.decode(output[0], skip_special_tokens=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I0z_d5naTpmq"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import uuid\n",
        "\n",
        "# Function for Gradio interface\n",
        "def chatbot_interface(pdf_file, query):\n",
        "    if pdf_file is None:\n",
        "        return \"Please upload a PDF file first.\"\n",
        "\n",
        "    pdf_path = pdf_file.name\n",
        "\n",
        "    # Re-process the PDF for each new upload\n",
        "    # 1. Extract text\n",
        "    document_text = extract_text(pdf_path)\n",
        "\n",
        "    # 2. Chunk text\n",
        "    chunks = chunk_text(document_text)\n",
        "    print(f\"Processed {len(chunks)} chunks from the uploaded PDF.\")\n",
        "\n",
        "    # 3. Embed chunks\n",
        "    embeddings = embedder.encode(chunks).astype(np.float32)\n",
        "\n",
        "    # 4. Prepare vectors for Pinecone\n",
        "    vectors = []\n",
        "    for i, emb in enumerate(embeddings):\n",
        "        vectors.append({\n",
        "            \"id\": str(uuid.uuid4()),\n",
        "            \"values\": emb.tolist(),\n",
        "            \"metadata\": {\"text\": chunks[i]}\n",
        "        })\n",
        "\n",
        "    # 5. Clear previous data in Pinecone and upload new vectors\n",
        "    print(\"Clearing Pinecone index...\")\n",
        "    index.delete(delete_all=True, namespace=\"\") # Clear the entire index\n",
        "    print(f\"Upserting {len(vectors)} new vectors to Pinecone...\")\n",
        "    index.upsert(vectors)\n",
        "    print(\"PDF processed and indexed successfully!\")\n",
        "\n",
        "    # Now answer the question using the newly indexed document\n",
        "    answer = answer_question(query)\n",
        "\n",
        "    return answer\n",
        "\n",
        "# Launch Gradio\n",
        "iface = gr.Interface(\n",
        "    fn=chatbot_interface,\n",
        "    inputs=[gr.File(label=\"Upload PDF\"), gr.Textbox(label=\"Your Query\")],\n",
        "    outputs=\"text\",\n",
        "    title=\"PDF RAG Chatbot\"\n",
        ")\n",
        "iface.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}